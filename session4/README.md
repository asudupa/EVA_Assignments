
Session 4 Assignment:
Topics covered so far...
Mark:<br/>
     1. **How many layers**: Typically number of conv layers are deribed from input image resolution. With 3x3 kernel, number of layers goes upto a receptive field of 7x7 is reached before final activation layer. Further adding Conv layer would not help in capturing the features. Note that each Maxpool layer reduces the image by two.<br/> <br/>
     2. **MaxPooling**: Maxpooling is typically done by 2x2 kernel, convolues over the input and select maximum value in each convolution. That is in each set of 2x2, it pick the maximum value and that 2x2 pixels is represented by Max value. Thus input image size reduces by half in output.<br/> <br/>
     3. **1x1 Convolutions**: It is convolution by 1x1xd kernel. Where d represent the depth or numbmer of channels. 1x1 transfer the input value as-is across the channel, with each channel has its own weight. Thus in spatial dimension information is passed on as is but at channel level weight determines (through back propagation) if the entire feature map (Channel) extracted by kernel should be passed on to next layer or not (would be reduced to negative value so that it gets filtered out during activation).<br/> <br/>
     4. **3x3 Convolutions**: It is a Convolution over input image with 3x3 Kernel. 3x3 kernel convolves over input image multiplies input values with kernel values and sums it up to represent the 3x3 image pixels. In general 3x3 size kernel is preferred over other size as: 1- odd numbered kernel is used to avoid waste of computational capacity, 2- Beign the lowest (odd sized) kernel size reduces number of parameter for computation. Two layer of 3x3 ((9+9 =18) is lesser than one 5x5 (5x5=25). 3-Hardware is optimized for 3x3 kernel and hence computation with 3x3 is much faster than other kernel size. <br/> <br/>
     5. **Receptive Field**: It is the spatial area of the input image which kernel is viewing. With number of layers, receptive each pixel of present layer would see wider area of previous layers, forming a pyramid shape, with top most layer seeing most of the image. For example for an input image of size 28x28x1, and the kernel of 3x3, 9 (3x3) pixels of input image forms one pixel of output image. However as kernel convolves over input image covering entire image (stride 1, padding 0), output image will of size 26x26. <br/><br/>
     6. **SoftMax**: It is one of the activation function, it is normalization over exponential value of input. For a input it calculates exponential value and divdes it by overall exponential value of all inputs. Output values will between 0 to 1 and sum of all output values will be 1. <br/><br/>
     7. **Learning Rate**: Learning rate is the hyper-parameter which controls the learning step value. It specifies by how much move in negative gradient slope. Higher the value bigger the step would be and viceersa. A low learning rate will slow down the movement towards minima and it takes lot of iteration to reach minima. While larger learning value will take bigger step towards minima, but will overshoot the minima and hence will oscilate. Learning rate can be set to varying values with larger values (bigger step) initially and lower value (smaller steps) as it reaches the minima. <br/><br/>
     8. **Kernels and how do we decide the number of kernels?**: Kernel is a matrix of specific size, which when convolved over input image, filters as per kernel values. They help in extracting the features; initially low level edges and progressively complex features like patterns, features etc. Each channel in the layer have a set of filter values, initially set randomly, and learning progresses back-propagation will reset the value based on the feedback.<br/>
Number of kernels depend on the resolution of the input image and size of the feature to be extracted. By default object size is set equal to image size, so that the final layer sees all of the pixels in the image. <br/><br/>
     9. **Batch Normalization**: It is channel wise normalization of the input image over entire batch. Batch normalization helps in removing the extreme values in a input image or for a class, and avoid some of conditions like low lighting,. We know normalizing input image before convolution helps, in similiar way normalizing the values across a channel within the batch helps in faster learning and also learn for extreme input image values. Batchnormalization also helps in better learning witin layer across diffrent input in a batch. <br/><br/>
     10.**Image Normalization**: This is normalization of image with in each channels. Pixel values between 0 to 255 is normalized between 0 to 1. This helps in overt influence of large values. Image can be normalized in 3 ways, Mean normalization, normalization between minimum and maximum values, and normalization by Standard Deviation. <br/><br/>
     11.**Position of MaxPooling**: There is no clear evidence or detailed study on position of Maxpooling layer in the model. Like before dropout, if after Conv, etc. Since Maxpooling is spatial manipulation, dropout which is droping whole neuron would not matter. It is positioned as per each model needs. <br/><br/>
     12.**Concept of Transition Layers**: Transition layers are to simplify the network size. It concist of COnv+pooling, and reduce spatial information and takes in strongest feature. <br/><br/>
     13.**Position of Transition Layer**:<br/><br/>
     14.**Number of Epochs and when to increase them**: Epochs are increased when metric; accuracy, shows signs of further improvements in last few epochs. When accuracy does not show signs of saturation and keeps moving towards huigher value (might be sinusoidal), it makes sense to increase Epochs<br/><br/>
     15.**DropOut**: Dropout is way to avoid over fitting on training data. In dropout randomly specefied neurons are switched off, i.e. its output is 0. Every time it will be a different neuron that gets switched off. Thus model cannot rely on particular neurons and memorize the path. When compared to training run, test run will have that much (dropout value) additional neurons. <br/><br/>
     16.**When do we introduce DropOut, or when do we know we have some overfitting**: Dropout is used when model is overfitting. When training accuracy is very high but validation or test accuracy is low, it means model has memorized on training data, memorized the path and hence shows high accuracy but on a new data (validation/test data) it fails. High training accuracy but low validation/test accuracy is clear indicator of over fitting and time apply dropout. <br/><br/>
     17.**The distance of MaxPooling from Prediction**:Maxpooling helps in reducing spatial dimension and thereby parameters. It is typically positioned inbetween two Conv layers. There is no clarity on how Maxpooling could impact if positioned just before prediction layer.<br/><br/>
     18.**The distance of Batch Normalization from Prediction**:Again there is no clear indicators on positioning BatchNormalization layer just before prediction. Typically normalization helps before Conv layer or a Conv layer follows normalization. <br/><br/>
     19.**When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)**: Depends on input image resolution size. When input image has objects or higher resolution; say 11x11 then it would be ideal to go until 11x11 instead of standard 3x3.
     20.**How do we know our network is not going well, comparatively, very early**:Initial few epochs indicate the direction of the model output. If they are very low, not much could improve in following epochs. SO if in first few epochs, the metric is high, then the chances of sucessfull model is high. <br/><br/>
     21.**Batch Size, and effects of batch size**: Batch size is the number of input images fed into the GPU. GPU processes all of these input simultaneously. Higher the batch size better is the accuracy and fastr is the training. However batch size is limited by the hardware capacity. <br/><br/>
     22.**When to add validation checks**:Validation check helps us understand how model lis performing on test or validation data. Once the intial model is good on training data, it is better to include validation checks.<br/><br/>
     23.**LR schedule and concept behind it**: Learning rate schedule helps in higher accuracy and learn faster. Varing LR helps in reducing the LR progressive;y as learning happens; high in intial state and lower in later states as learning stabilizes or reached minima. It typpically applies reducing LR in every epoch, from intial set value, reducing its value by specified amount. <br/><br/>
     24.**Adam vs SGD**: Both are optimization algorithms. They helps in minimizing the error function. SGD (Stochastic Gradient Descent) updates paramter for each training example, which makes it faster. Adam (Adaptive Moment Estimation) is variant og SGD, 
